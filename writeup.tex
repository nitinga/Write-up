\documentclass{article} % For LaTeX2e
\usepackage{nips15submit_e,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{amsmath}
%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09


\title{Singular Value Thresholding for Matrix Completion}


\author{
Sriram Ganesan\thanks{ The names are printed in alphabetical order by last name.} \\
\texttt{---} \\
\And
Nitin Garg \\
\texttt{gargn@umich.edu} \\
\AND
Jeeheh Oh \\
\texttt{jeeheh@umich.edu} \\
\And
Jonathan Stroud \\
\texttt{stroud@umich.edu} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}


\maketitle

\begin{abstract}
%\input{abstract}
\end{abstract}

\section{Introduction}

\emph{Collaborative Filtering} is the process of learning patterns by
aggregating information over multiple sources of data, typically
multiple users of the same system. These techniques have been applied
effectively on many difficult problems, most notably on recommender
systems such as those found on Netflix, Amazon, Spotify, and others.
The goal of these systems is to produce targeted product
recommendations for individual users given the products the user has
previously viewed or rated. Users have typically viewed or rated only
a small subset of the available products, and different users
typically have not rated the same set of products. These problems do
not easily lie into any supervised learning framework, so we instead
employ the framework of \emph{matrix completion}.

In matrix completion, we reconstruct a matrix $M$ from a set of known
entries $M_{ij}, \ (i,j) \in \Omega$. Relying on the assumption that
$M$ has low rank, we can pose this as a constrained rank-minimization
problem
\begin{equation*}
\begin{aligned}
  & \underset{X}{\text{minimize}} & & \mathrm{rank}(X) \\
  & \text{subject to}             & & X_{ij} = M_{ij}, \ \ \forall (i, j)
  \in \Omega. \\
\end{aligned}
\end{equation*}

In this project, we explore several algorithms for matrix completion
via rank-minimization and compare their performance on collaborative
filtering baselines. We provide an implementation of the method
described in ``A Singular Value Thresholding Algorithm for Matrix
Completion'' \cite{cai2010singular} and reproduce their experiments.
We also provide an implementation of a competing algorithm from
``Matrix Completion from a Few Entries'' \cite{keshavan2010matrix} and
compare their effectiveness.


\section{Different Algorithms}

\subsection{Eigentaste}

Eigentaste is a collaborative filtering algorithm which applies
principal component analysis to the dense subset of user ratings, thus
facilitating dimensionality reduction for offline clusters and rapid
computation of recommendations. Mean rating of the jth item in the
gauge set is given by
\begin{equation*}
\mu_{ij}=\frac{1}{n}{\sum_{i\epsilon U_{j}}}\widetilde{r}_{ij}\\
\end{equation*}
\begin{equation*}
\sigma_j^2=\frac{1}{n-1}{\sum_{i\epsilon U_{j}}}({\widetilde{r}_{ij}-\mu_{j}})^{2}
\end{equation*}

In A, the normalized rating $r_{ij}$ is set to
$({\widetilde{r}_{ij}-\mu_{j}})/\sigma _{j}$ . The global correlation
matrix is given by
\begin{equation*}
C=\frac{1}{n-1}A^{T}A=E^{T} \Lambda E
\end{equation*}

The data is projected along the first v eigenvectors $x=R{E_{v}}^{T}$

\textit{Recursive Rectangular Clustering: }

\begin{enumerate}
\item Find the minimal rectangular cell that encloses all the points
  in the eigenplane.
\item Bisect along x and y axis to form 4 rectangular sub-cells.
\item Bisect the cells in step 2 with origin as a vertex to form
  sub-cells at next hierarchial level.
\end{enumerate}

 \textit{Online Computation of Recommendations}

\begin{enumerate}
\item Collect ratings of all items in gauge set.
\item Use PCA to project this vector to eigenplane.
\item Find the representative cluster.
\item Look up appropriate recommendations, present them to the new
  user, and collect ratings.
\end{enumerate}


\subsection{Spectral Matrix Completion}
%\input{smc}

\subsection{Singular Value Thresholding}

Singular Value Thresholding (SVT) \cite{cai2010singular} is an
algorithm proposed for \emph{nuclear norm minimization} of a matrix.
Formally, SVT addresses the optimization problem
\begin{equation*}
\begin{aligned}
  & \underset{X}{\text{minimize}} & & \|X\|_{*} \\
  & \text{subject to}             & & \mathcal{P}_\Omega (X) =
  \mathcal{P}_\Omega (M), \\
\end{aligned}
\end{equation*}
where $\|\cdot\|_{*}$ is the \emph{nuclear norm}, or the sum of the
singular values and $\mathcal{P}_\Omega (\cdot)$ makes zero all
entries $(i, j) \notin \Omega$. This can be thought of as a convex
relaxation to the rank minimization problem, and the two are formally
equivalent under some conditions. The rank minimization problem is,
however, highly non-convex and therefore not a suitable candidate for
black-box optimization algorithms.

Singular Value Thresholding works by iteratively constructing $X$
using a low-rank, low-singular value approximation to an auxiliary
sparse matrix $Y$. $Y$ is then adjusted to ensure the resulting
approximation in the subsequent step has matching entries
$X_{ij} = M_{ij}$. Each iteration consists of the inductive steps
\begin{equation*}
\begin{cases}
X^{k} = \mathrm{shrink}(Y^{k-1}, \tau) \\
Y^{k} = Y^{k-1} + \delta_k \mathcal{P}_\Omega (M-X^{k}),              \\
\end{cases}
\end{equation*}
where $\mathrm{shrink}(\cdot, \cdot)$ is the \emph{singular value
  shrinkage operator}. Given a singular value decomposition $X = U
\Sigma V^T$, $\Sigma = \mathrm{diag}(\{\sigma_i\}_{1 \le i \le r})$, we
  can write this as
\begin{equation*}
\mathrm{shrink}(X, \tau) = U\Sigma_\tau V^T, \ \ \Sigma_\tau = \mathrm{diag}(\{(\sigma_i-\tau)_{+}\}).
\end{equation*} 

These two operations, when repeated, approach a low-nuclear norm
solution by repeatedly shrinking the singular values of X. This
algorithm has shown success in recovering accurate low-rank solutions
when the source of $M$ is also low-rank, even though it does not
optimize this objective directly. The original authors discuss its
theoretical guarantees in detail, but we choose to omit them in this
discussion.

In practice, this system has a number of hyperparameters that must be
carefully tuned to guarantee convergence. The shrinkage value $\tau$
must be set fairly high in order for the algorithm to converge
quickly, but not too high that it dwarfs the true singular values. The
stepsizes $\delta_k$ are similarly sensitive. These can be set
dynamically as well, though we choose to maintain a fixed stepsize
throughout. We compute the decomposition of $Y^K$ in batches, which
introduces a new batch size parameter $l$. Also important is the
initialization of $Y^0$, for which the authors provide helpful
strategies. Finally, we use the relative error
$\|\mathcal{P}_{\Omega}(X^k-M)\|_F / \|P_{\Omega} (M)\|$ as a stopping
criterion. We terminate when this drops below a small $\epsilon$.



\section{Results and Discussion}
\subsection{Synthetic Data}

% I put this table in a separate file because it is auto-generated by
% a script. I'll clean it up later once I finish running the
% experiments - Jonathan
\input{tables/svt_synth.tex}

\subsection{Jester Dataset}

\section{Conclusions}

\section{Future Work}


\bibliographystyle{plain}
\bibliography{writeup}

\end{document}
