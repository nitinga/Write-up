\documentclass{article} % For LaTeX2e
\usepackage{nips15submit_e,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{amsmath}
%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09


\title{Singular Value Thresholding for Matrix Completion}


\author{
Sriram Ganesan\thanks{ The names are printed in alphabetical order by last name.} \\
\texttt{sriramg@umich.edu} \\
\And
Nitin Garg \\
\texttt{gargn@umich.edu} \\
\AND
Jeeheh Oh \\
\texttt{jeeheh@umich.edu} \\
\And
Jonathan Stroud \\
\texttt{stroud@umich.edu} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}


\maketitle

\begin{abstract}
\input{abstract}
\end{abstract}

\section{Introduction}

Recovering a low rank matrix from a given subset of its entries is a well known problem in collaborative filtering \cite{r25} , dimensionality reduction\cite{r20, r28} and multi-class learning \cite{r2, r22}, finding the lowest rank matrix satisfying equality constraints is NP-hard. The complexity class of decision problems that are intrinsically harder than those that can be solved by a nondeterministic Turing machine in polynomial time. Examples of NP-hard problems include the Hamiltonian cycle and traveling salesman problems. All known algorithms which can compute lowest rank solution for all instances require time atleast exponential in the dimensions of the matrix in both theory and practice.

Candes and Recht showed that most low rank matrices could be recovered from most sufficiently large sets of entries by computing the matrix of minimum nuclear norm that agreed with the provided entries\cite{r4} and the known set of entries could comprise a vanishing fraction of the entire matrix. The nuclear norm is equal to the sum of the singular values of a matrix and is the best convex lower bound of the rank function on the set of matrices whose singular values are all bounded by 1. The intuition behind this measure is that whereas the rank function counts the number of nonvanishing singular values, the nuclear norm sums their amplitude, much like how the `1 norm is a useful substitute for counting the number of nonzeros in a vector. Moreover, the nuclear norm can be minimized subject to equality constraints via semidefinite programming.

Nuclear norm minimization had long been observed to produce very low-rank solutions in practice (see, for example \cite{r11, r12, r26}), but only very recently was there any theoretical basis for when it produced the minimum rank solution. The first paper to provide such foundations was \cite{r24}, where Recht, Fazel, and Parrilo developed probabilistic techniques to study average case behavior and showed that the nuclear norm heuristic could solve most instances of the rank minimization problem assuming the number of linear constraints was sufficiently large. The results in \cite{r24} inspired a groundswell of interest in theoretical guarantees for rank minimization, and these results lay the foundation for \cite{r4}. Candes and Recht’s bounds were subsequently improved by Candes and Tao \cite{r7} and Keshavan, Montanari, and Oh \cite{r18} to show that one could, in special cases, reconstruct a low-rank matrix by observing a set of entries of size at most a polylogarithmic factor larger than the intrinsic dimension of the variety of rank r matrices.

\emph{Collaborative Filtering} is the process of learning patterns by
aggregating information over multiple sources of data, typically
multiple users of the same system. These techniques have been applied
effectively on many difficult problems, most notably on recommender
systems such as those found on Netflix, Amazon, Spotify, and others.
The goal of these systems is to produce targeted product
recommendations for individual users given the products the user has
previously viewed or rated. Users have typically viewed or rated only
a small subset of the available products, and different users
typically have not rated the same set of products. These problems do
not easily lie into any supervised learning framework, so we instead
employ the framework of \emph{matrix completion}.

In matrix completion, we reconstruct a matrix $M$ from a set of known
entries $M_{ij}, \ (i,j) \in \Omega$. Relying on the assumption that
$M$ has low rank, we can pose this as a constrained rank-minimization
problem
\begin{equation*}
\begin{aligned}
  & \underset{X}{\text{minimize}} & & \mathrm{rank}(X) \\
  & \text{subject to}             & & X_{ij} = M_{ij}, \ \ \forall (i, j)
  \in \Omega. \\
\end{aligned}
\end{equation*}

In this project, we explore several algorithms for matrix completion
via rank-minimization and compare their performance on collaborative
filtering baselines. We provide an implementation of the method
described in ``A Singular Value Thresholding Algorithm for Matrix
Completion'' \cite{cai2010singular} and reproduce their experiments.
We also provide an implementation of a competing algorithm from
``Matrix Completion from a Few Entries'' \cite{keshavan2010matrix} and
compare their effectiveness.



\section{Singular Value Thresholding}

Singular Value Thresholding (SVT) \cite{cai2010singular} is an
algorithm proposed for \emph{nuclear norm minimization} of a matrix.
Formally, SVT addresses the optimization problem
\begin{equation*}
\begin{aligned}
  & \underset{X}{\text{minimize}} & & \|X\|_{*} \\
  & \text{subject to}             & & \mathcal{P}_\Omega (X) =
  \mathcal{P}_\Omega (M), \\
\end{aligned}
\end{equation*}
where $\|\cdot\|_{*}$ is the \emph{nuclear norm}, or the sum of the
singular values and $\mathcal{P}_\Omega (\cdot)$ makes zero all
entries $(i, j) \notin \Omega$. This can be thought of as a convex
relaxation to the rank minimization problem, and the two are formally
equivalent under some conditions. The rank minimization problem is,
however, highly non-convex and therefore not a suitable candidate for
black-box optimization algorithms.

Singular Value Thresholding works by iteratively constructing $X$
using a low-rank, low-singular value approximation to an auxiliary
sparse matrix $Y$. $Y$ is then adjusted to ensure the resulting
approximation in the subsequent step has matching entries
$X_{ij} = M_{ij}$. Each iteration consists of the inductive steps
\begin{equation*}
\begin{cases}
X^{k} = \mathrm{shrink}(Y^{k-1}, \tau) \\
Y^{k} = Y^{k-1} + \delta_k \mathcal{P}_\Omega (M-X^{k}),              \\
\end{cases}
\end{equation*}
where $\mathrm{shrink}(\cdot, \cdot)$ is the \emph{singular value
  shrinkage operator}. Given a singular value decomposition $X = U
\Sigma V^T$, $\Sigma = \mathrm{diag}(\{\sigma_i\}_{1 \le i \le r})$, we
  can write this as
\begin{equation*}
\mathrm{shrink}(X, \tau) = U\Sigma_\tau V^T, \ \ \Sigma_\tau = \mathrm{diag}(\{(\sigma_i-\tau)_{+}\}).
\end{equation*} 

These two operations, when repeated, approach a low-nuclear norm
solution by repeatedly shrinking the singular values of X. This
algorithm has shown success in recovering accurate low-rank solutions
when the source of $M$ is also low-rank, even though it does not
optimize this objective directly. The original authors discuss its
theoretical guarantees in detail, but we choose to omit them in this
discussion.

In practice, this system has a number of hyperparameters that must be
carefully tuned to guarantee convergence. The shrinkage value $\tau$
must be set fairly high in order for the algorithm to converge
quickly, but not too high that it dwarfs the true singular values. The
stepsizes $\delta_k$ are similarly sensitive. These can be set
dynamically as well, though we choose to maintain a fixed stepsize
throughout. We compute the decomposition of $Y^K$ in batches, which
introduces a new batch size parameter $l$. Also important is the
initialization of $Y^0$, for which the authors provide helpful
strategies. Finally, we use the relative error
$\|\mathcal{P}_{\Omega}(X^k-M)\|_F / \|P_{\Omega} (M)\|$ as a stopping
criterion. We terminate when this drops below a small $\epsilon$.

\section{Spectral Matrix Completion}

\input{smc}


\section{Baseline Algorithms}

\subsection{Eigentaste}

Eigentaste is a collaborative filtering algorithm which applies
principal component analysis to the dense subset of user ratings, thus
facilitating dimensionality reduction for offline clusters and rapid
computation of recommendations. Mean rating of the jth item in the
gauge set is given by
\begin{equation*}
\mu_{ij}=\frac{1}{n}{\sum_{i\epsilon U_{j}}}\widetilde{r}_{ij}\\
\end{equation*}
\begin{equation*}
\sigma_j^2=\frac{1}{n-1}{\sum_{i\epsilon U_{j}}}({\widetilde{r}_{ij}-\mu_{j}})^{2}
\end{equation*}

In A, the normalized rating $r_{ij}$ is set to
$({\widetilde{r}_{ij}-\mu_{j}})/\sigma _{j}$ . The global correlation
matrix is given by
\begin{equation*}
C=\frac{1}{n-1}A^{T}A=E^{T} \Lambda E
\end{equation*}

The data is projected along the first v eigenvectors $x=R{E_{v}}^{T}$

\textit{Recursive Rectangular Clustering: }

\begin{enumerate}
\item Find the minimal rectangular cell that encloses all the points
  in the eigenplane.
\item Bisect along x and y axis to form 4 rectangular sub-cells.
\item Bisect the cells in step 2 with origin as a vertex to form
  sub-cells at next hierarchial level.
\end{enumerate}

 \textit{Online Computation of Recommendations}

\begin{enumerate}
\item Collect ratings of all items in gauge set.
\item Use PCA to project this vector to eigenplane.
\item Find the representative cluster.
\item Look up appropriate recommendations, present them to the new
  user, and collect ratings.
\end{enumerate}

\subsection{K-Nearest Neighbor}
Nearest neighbor algotihm predicts the ratings based on mean user rating and variation of the current rating from the mean rating of its nearest neighbors.
\begin{equation*}
p_{ij}=\overline{r_{i}}+\kappa\sum_{k=1}^n w(i,p)(\overline{r_{pj}}-\overline{r_{p}})
\end{equation*}
where $\overline{r_{i}}$ is the average joke rating for user i, and $\kappa$ is a
normalizing factor ensuring that the absolute value of the
weights sum to 1. We implemented the weighted nearest neighbor algorithm.
We used a function of Euclidean distance from
user i to user p as the weight w(i, p), and $\kappa$ = $\sum_{k=1}^n w(i,p)$.
Specifically, if we are interested in q nearest neighbors,
w(i; p) = d(i, q +1)- d(i, p). This ensures that i’s closest
neighbor has the largest weight.

\section{Experimental Results}
\subsection{Synthetic Data}
The performance of the SVT and SMC algorithms were compared on synthetically generated matrices of varying ranks, sizes and sparsity. Specifically, the algorithms were tested on all combinations of matrix size(1000, 5000), rank (5, 10 and 20), and sparsity(30\%, 50\% and 70\%). As a baseline to SVT and SMC, a column mean matrix completion method was implemented. This method predicts the average column value for all missing entries. 
In the Tables~\ref{MAE},~\ref{Time} the mean absolute error and the run time is averaged over the categories of size, rank and sparsity. For example, out of the eighteen tests that were run, nine tests included a matrix of size 1000x1000. Therefore the mean absolute error value for all nine tests are averaged in order to represent the values in the size 1000 column of the table below. Mean absolute error is the absolute value of the average error on the unknown entries. Run time is measured in seconds and all simulations were run at University of Michigan's CAEN lab computers (8GB memory with one core i7 processors). The columns in Tables~\ref{MAE},~\ref{Time} contain the averaged mean absolute error and the run time for all experiments.

\begin{table} [ht!]
\centering
 \caption{Mean Absolute Error}
 \begin{tabular}{l @{\hspace{12pt}}| l @{\hspace{12pt}}l @{\hspace{12pt}}l @{\hspace{12pt}}|l @{\hspace{12pt}}l @{\hspace{12pt}}l @{\hspace{12pt}}|l @{\hspace{12pt}}l @{\hspace{12pt}}}% p{2cm}}
  \hline \hline
   & &\text{Rank}  & &  &\text{Sparsity}&& \text{Size} & \text{(Average)} \\
\text{Algorithm} & 5 & 10 & 20 & 30\% & 50\% & 70\% & 1000 & 5000 \\
\hline
\text{Mean}  & 5.65 & 9.90 & 13.98 & 9.10 & 10.21 & 10.21 & 9.43 & 10.26\\
\text{SVT}  & 2.05 & 7.2E-04 & 1.4E-03 & 2.05 & 8.4E-04 & 9.6E-04 & 1.37 & 7.6E-04\\
\text{SMC} & 2.1E-04 & 4.3E-07 & 4.5E-07 & 2.1E-04 & 4.2E-07 & 3.8E-07  & 1.4E-04 & 3.7E-07\\
 \hline \hline
\label{MAE}
 \end{tabular}
\end{table}

\begin{table} [ht!]
\centering
 \caption{Run Time (in seconds)}
 \begin{tabular}{l @{\hspace{12pt}}| l @{\hspace{12pt}}l @{\hspace{12pt}}l @{\hspace{12pt}}|l @{\hspace{12pt}}l @{\hspace{12pt}}l @{\hspace{12pt}}|l @{\hspace{12pt}}l @{\hspace{12pt}}}% p{2cm}}
  \hline \hline
  & &\text{Rank}  & &  &\text{Sparsity}&&\text{Size} & \text{(Average)} \\
\text{Algorithm}& 5 & 10 & 20 & 30\% & 50\% & 70\%  & 1000 & 5000 \\
\hline
\text{Mean}  & 0.12 & 0.12 & 0.10 & 0.11 & 0.11 & 0.12 & 0.01 & 0.22\\
\text{SVT} & 26 & 39 & 92 & 42 & 54 & 60& 6 & 98 \\
\text{SMC}& 252 & 405 & 1623 & 1021 & 510 & 748 & 86 & 1433 \\
\hline \hline
\label{Time}
 \end{tabular}
\end{table}

Both SVT and SMC outperform the baseline column-mean method. In addition, SMC consistently outperforms SVT in terms of mean absolute error, by order of three. While SMC significantly outperforms SVT in terms of accuracy, SVT significantly outperforms SMC in terms of time.  It appears that the computational cost of SMC grows exponentially with rank size. Even at rank five, the smallest tested rank value, SMC runs 9.6 times slower than SVT. 

% I put this table in a separate file because it is auto-generated by
% a script. I'll clean it up later once I finish running the
% experiments - Jonathan
\input{tables/svt_synth.tex}

\subsection{Jester Dataset}
The Jester Dataset~\cite{Goldberg2012} is used to evaluate our implementation of various algorithms. This dataset is wellknown and has several variants with different levels of difficulty. In the present paper, Jester dataset 1 is being used, which contains anonymous rating data from 73,421 users. The ratings are real values ranging from -10.00 to +10.00 and the unrated values are marked as "NaN". There is one row per user and 100 columns for 100 jokes. There are also 10 columns/Jokes which are dense, i.e. almost all users have rated those jokes. These 10 dense columns becomes very critical for some algorithms, such as Eigentaste.

\section{Conclusions}

\section{Future Work}


\bibliographystyle{plain}
\bibliography{writeup}

\end{document}
