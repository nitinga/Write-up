\documentclass{article} % For LaTeX2e
\usepackage{nips15submit_e,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{amsmath}
%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09


\title{Singular Value Thresholding for Matrix Completion}


\author{
Sriram Ganesan\thanks{ The names are printed in alphabetical order by last name.} \\
\texttt{---} \\
\And
Nitin Garg \\
\texttt{gargn@umich.edu} \\
\AND
Jeeheh Oh \\
\texttt{jeeheh@umich.edu} \\
\And
Jonathan Stroud \\
\texttt{stroud@umich.edu} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}


\maketitle

\begin{abstract}
\input{abstract}
\end{abstract}

\section{Introduction}

\section{Different Algorithms}

\subsection{Baseline Algorithm (Eigentaste)}

\subsection{SMC}
\input{smc}

\subsection{Singular Value Thresholding}

Singular Value Thresholding (SVT) \cite{cai2010singular} is an
algorithm proposed for \emph{nuclear norm minimization} of a matrix
$X$ from a few known entries $M_{ij}, (i,j) \in \Omega$. Formally, SVT
addresses the optimization problem

\begin{equation*}
\begin{aligned}
  & \underset{X}{\text{minimize}} & & \|X\|_{*} \\
  & \text{subject to}             & & \mathcal{P}_\Omega (X) =
  \mathcal{P}_\Omega (M), \\
\end{aligned}
\end{equation*}

where $\|\cdot\|_{*}$ is the \emph{nuclear norm}, or the sum of the
singular values and $\mathcal{P}_\Omega (\cdot)$ makes zero all
entries $(i, j) \notin \Omega$. This can be thought of as a convex
relaxation to the rank minimization problem, and the two are formally
equivalent under some conditions. The rank minimization problem is,
however, highly non-convex and therefore not a suitable candidate for
black-box optimization algorithms.

Singular Value Thresholding works by iteratively constructing $X$
using a low-rank, low-singular value approximation to an auxiliary
sparse matrix $Y$. $Y$ is then adjusted to ensure the resulting
approximation in the subsequent step has matching entries
$X_{ij} = M_{ij}$. Each iteration consists of the inductive steps

\begin{equation*}
\begin{cases}
X^{k} = \mathrm{shrink}(Y^{k-1}, \tau) \\
Y^{k} = Y^{k-1} + \delta_k \mathcal{P}_\Omega (M-X^{k}),              \\
\end{cases}
\end{equation*}

where $\mathrm{shrink}(\cdot, \cdot)$ is the \emph{singular value
  shrinkage operator}. Given a singular value decomposition $X = U
\Sigma V^T$, $\Sigma = \mathrm{diag}(\{\sigma_i\}_{1 \le i \le r})$, we
  can write this as

\begin{equation*}
\mathrm{shrink}(X, \tau) = U\Sigma_\tau V^T, \ \ \Sigma_\tau = \mathrm{diag}(\{(\sigma_i-\tau)_{+}\}).
\end{equation*} 

These two operations, when repeated, approach a low-nuclear norm
solution by repeatedly shrinking the singular values of X. This
algorithm has shown success in recovering accurate low-rank solutions
when the source of $M$ is also low-rank, even though it does not
optimize this objective directly. The original authors discuss its
theoretical guarantees in detail, but we choose to omit them in this
discussion.

In practice, this system has a number of hyperparameters that must be
carefully tuned to guarantee convergence. The shrinkage value $\tau$
must be set fairly high in order for the algorithm to converge
quickly, but not too high that it dwarfs the true singular values. The
stepsizes $\delta_k$ are similarly sensitive. These can be set
dynamically as well, though we choose to maintain a fixed stepsize
throughout. We compute the decomposition of $Y^K$ in batches, which
introduces a new batch size parameter $l$. Also important is the
initialization of $Y^0$, for which the authors provide helpful
strategies. Finally, we use the relative error

$\|\mathcal{P}_{\Omega}(X^k-M)\|_F / \|P_{\Omega} (M)\|$ as a stopping
criterion; we terminate when this drops below a small $\epsilon$.



\section{Results and Discussion}
\subsection{Synthetic Data}

\input{svt_synth.tex}

\subsection{Jester Dataset}

\section{Conclusions}

\section{Future Work}


\bibliographystyle{plain}
\bibliography{writeup}

\end{document}
